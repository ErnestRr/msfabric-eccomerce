{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b9d05-84fd-4fa4-9966-342115028c52",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-01-05T22:34:08.391137Z",
       "execution_start_time": "2026-01-05T22:33:29.0492553Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "86baa602-b791-4de3-bbb7-551dc9c9c3fa",
       "queued_time": "2026-01-05T22:33:19.1272115Z",
       "session_id": "229460d5-2a77-415f-81f4-a035fe5a3cb4",
       "session_start_time": "2026-01-05T22:33:19.1282686Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 229460d5-2a77-415f-81f4-a035fe5a3cb4, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Procesando tabla: ordenes ---\n",
      "ÉXITO: bronze_ordenes cargada con 1200 registros.\n",
      "--- Procesando tabla: order_items ---\n",
      "ÉXITO: bronze_order_items cargada con 1200 registros.\n",
      "--- Procesando tabla: logistica_investigacion ---\n",
      "ÉXITO: bronze_logistica_investigacion cargada con 1280 registros.\n",
      "--- Procesando tabla: productos ---\n",
      "ÉXITO: bronze_productos cargada con 15 registros.\n",
      "\n",
      "--- INGESTA A CAPA BRONZE FINALIZADA ---\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# PROJECT: E-commerce Logistics Analytics (Medallion Architecture)\n",
    "# LAYER: Bronze (Raw Data Ingestion)\n",
    "# SOURCE: Supabase (Transaction Pooler - IPv4) -> Fabric Lakehouse\n",
    "# =================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Parámetros de conexión basados en image_2eae6b\n",
    "# IMPORTANTE: Reemplaza 'TU_PASSWORD' con la contraseña que elegiste\n",
    "db_config = {\n",
    "    \"host\": \"------\", #\n",
    "    \"port\": \"----\",                               #\n",
    "    \"database\": \"postgres\",                       #\n",
    "    \"user\": \"-----\",      #\n",
    "    \"password\": \"-----\"                 \n",
    "}\n",
    "\n",
    "# 2. URL de conexión JDBC segura\n",
    "jdbc_url = f\"jdbc:postgresql://{db_config['host']}:{db_config['port']}/{db_config['database']}\"\n",
    "\n",
    "# 3. Lista de tablas a migrar\n",
    "# Estas son las tablas que contienen tus datos de 2023, 2024 y 2025\n",
    "source_tables = [\"ordenes\", \"order_items\", \"logistica_investigacion\", \"productos\"]\n",
    "\n",
    "def run_bronze_ingestion(table_name):\n",
    "    print(f\"--- Procesando tabla: {table_name} ---\")\n",
    "    try:\n",
    "        # Lectura desde Supabase\n",
    "        df = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", db_config['user']) \\\n",
    "            .option(\"password\", db_config['password']) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .load()\n",
    "        \n",
    "        # Guardado en el Lakehouse (Capa Bronze)\n",
    "        # Usamos mode('overwrite') para refrescar la data completa en esta etapa\n",
    "        target_table = f\"bronze_{table_name}\"\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)\n",
    "        \n",
    "        print(f\"ÉXITO: {target_table} cargada con {df.count()} registros.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: No se pudo procesar la tabla {table_name}.\")\n",
    "        print(f\"Detalle técnico: {str(e)}\")\n",
    "\n",
    "# 4. Ejecución del proceso para todas las tablas\n",
    "for table in source_tables:\n",
    "    run_bronze_ingestion(table)\n",
    "\n",
    "print(\"\\n--- INGESTA A CAPA BRONZE FINALIZADA ---\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "d45e3e12-0154-4222-b161-3dca4a50efe2",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "dc87f90b-f4fd-4bf6-8318-63ea5c7efb95",
    "known_lakehouses": [
     {
      "id": "d45e3e12-0154-4222-b161-3dca4a50efe2"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
